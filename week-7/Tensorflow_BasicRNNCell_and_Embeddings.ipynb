{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Basic RNN Cell usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "state_size = 37 # (parameter for the RNN cell; describes how wide the cell should be)\n",
    "embedding_size = 27 #(parameter for embedding layer; describes the size of embedding for each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_xs = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_ys = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_init_state = tf.placeholder(shape=[None, state_size], dtype=tf.float32, name=\"initial_state\")\n",
    "ph_batch_size = tf.placeholder(dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "The data in the files were populated using the *Data_Provider* notebook. *train_x* and *train_y* are both arrays of shape: [10,200], where 200 is a fixed sequence length for this model and 10 is the total number of samples we have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "\n",
    "def get_train_batches(train_x, train_y, batch_size):\n",
    "    for i in range(0, train_x.shape[0], batch_size):    \n",
    "        yield train_x[i : i+batch_size], train_y[i : i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some essential utilites from Data_Provider.py notebook\n",
    "import pickle\n",
    "vocab_to_int = pickle.load(open('vocab_to_int.txt','rb'))\n",
    "int_to_vocab = pickle.load(open('int_to_vocab.txt','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Given an input data of shape: [batch_size, seq_length], rnn_inputs returns a tensor of shape: [batch_size, seq_length, embedding_size]. This is purely driven by our decision to use an embedding matrix to represent each token (characters). Otherwise, we would've had to one-hot encode each character, and modify our input layer. I'm not sure what the complexity of this task would need to be, but using embedding matrices seem to be the preferred approach in most NLP applications of neural network. I think it massively improves the underlying computational complexity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of unique characters.. you'd normally do this by inspecting the data directly\n",
    "num_classes = 83\n",
    "embeddings = tf.Variable(initial_value=tf.random_normal(mean=0., stddev=0.1, shape=[num_classes,embedding_size]))\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, ph_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RNN layer using the Tensorflow BasicRNNCell API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = rnn_cell.zero_state(ph_batch_size, tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(rnn_cell, rnn_inputs, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output from RNN Cell\n",
    "- outputs:  [batch_size, Seq_length, state_size]\n",
    "We need to collapse this to [batch_size x seq_length, state_size] to further process it.\n",
    "\n",
    "- final_state:  [batch_size, state_size] \n",
    "The state is returned only for the *final* time-step. This is what we're interested in primarily during text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_reshaped = tf.reshape(outputs, [-1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(inputs=outputs_reshaped, units=num_classes, activation=None)\n",
    "# The shape of predictions will be: [seqlen * batch_size, num_classes]\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 83]\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "ph_ys is shaped as follows: [batch_size, seq_length]. In order to use it against logits obtained by multiplying hidden_states (states) and output matrix (V), we need to collapse it into a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basically flatted ys into a flat vector\n",
    "ph_ys_reshaped = tf.reshape(ph_ys, shape=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    }
   ],
   "source": [
    "print(ph_ys_reshaped.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow method: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "is very interesting. It (the method) will basically look at (the integer representation of) each character and evaluate a one-hot representation based on its label. Thus, ph_ys_reshaped, of size [batch_size * seqlen] will effectively be unrolled into a vector of length [batch_size x seqlen x num_classes]. The method then uses this unrolled vector to evaluate the loss against *logits*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=ph_ys_reshaped, logits=logits)\n",
    "loss = tf.reduce_mean(losses)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0 , loss evaluated:  2.32801\n",
      "epochs:  1 , loss evaluated:  2.14393\n",
      "epochs:  2 , loss evaluated:  2.05178\n",
      "epochs:  3 , loss evaluated:  2.01087\n",
      "epochs:  4 , loss evaluated:  1.98479\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 # Take 100 at a time. We have 10,000 needed, so 100 iterations per epochs\n",
    "seq_length = 100 # each vector is fixed length of 100 (by inspecting data)\n",
    "chkpt_path = \"ckpts/\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_loss = 0.\n",
    "    n_epochs = 5\n",
    "    for i in range(n_epochs):\n",
    "        for xs, ys in get_train_batches(train_x, train_y, batch_size=100):\n",
    "            _, train_loss_val = sess.run([train_op, loss], \n",
    "                                         feed_dict={ph_xs: xs, \n",
    "                                                    ph_ys: ys,\n",
    "                                                    ph_init_state: np.zeros([batch_size, state_size]),\n",
    "                                                    ph_batch_size: batch_size\n",
    "                                                   })\n",
    "        print(\"epochs: \",i, \", loss evaluated: \",train_loss_val)\n",
    "\n",
    "    # Save the model at the end of the run.\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, chkpt_path+\"tensorflow_BasicRNNCell.ckpt\", global_step=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_session(sess):\n",
    "    ckpt = tf.train.get_checkpoint_state(chkpt_path)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(\"restoring model from \",ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printChars(chars):\n",
    "    print('------- Generated Text ----------')\n",
    "    print(''.join(str(c) for c in chars))\n",
    "    print('-------      END        ---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring model from  ckpts/tensorflow_BasicRNNCell.ckpt-5\n",
      "------- Generated Text ----------\n",
      "21 hotad y we ban'tr\n",
      "alid ad bfoudaskf he o tinys anerere and abcho\n",
      "ssktrhawhowhere hSinyesh\" g a an w\n",
      "-------      END        ---------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess = restore_session(sess)\n",
    "    \n",
    "    current_char = vocab_to_int['g']\n",
    "    num_chars = 100             # number of char sequence to try generate\n",
    "    chars = [current_char]     # some input char to get started with text generation\n",
    "    batch_size = 1             # generate 1 batch a time (i wonder if its possible to specify a larger batch)\n",
    "    state = np.zeros([batch_size, state_size]) # initial state to start off with\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        preds, state = sess.run([predictions, final_state], \n",
    "                                feed_dict={ph_xs: np.array(current_char).reshape([1,1]),\n",
    "                                           ph_init_state: state,\n",
    "                                           ph_batch_size: batch_size})\n",
    "        current_char = np.random.choice(preds.shape[-1], 1, p=np.squeeze(preds))[0]\n",
    "        chars.append(int_to_vocab[current_char])\n",
    "    printChars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
