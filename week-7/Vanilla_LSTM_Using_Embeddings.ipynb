{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LSTM\n",
    "### Based mostly on code and tutorial at\n",
    "\n",
    "(Someday I only hope I'll be able to write posts as well as the below)\n",
    "- https://github.com/suriyadeepan/rnn-from-scratch/blob/master/vanilla.py\n",
    "\n",
    "- http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n",
    "\n",
    "Some other resources to understand, especially, the origin of LSTMs and word-embeddings\n",
    "- https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "- http://sebastianruder.com/word-embeddings-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_size = 37 # (parameter for the RNN cell; describes how wide the cell should be)\n",
    "embedding_size = 18 #(parameter for embedding layer; describes the size of embedding for each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutable variables for weights and biases\n",
    "- W: weight matrices  for the recurrent relationship between cell states\n",
    "- U: weight matrices  for the connection between input and cell state\n",
    "- b: bias parameter for input -> cell state connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_i = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))\n",
    "W_f = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))\n",
    "W_o = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))\n",
    "W_g = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_i = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))\n",
    "U_f = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))\n",
    "U_o = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))\n",
    "U_g = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_xs = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_ys = tf.placeholder(shape=[None, None], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_init_state = tf.placeholder(shape=[None, state_size], dtype=tf.float32, name=\"initial_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "The data in the files were populated using the *Data_Provider* notebook. *train_x* and *train_y* are both arrays of shape: [10,200], where 200 is a fixed sequence length for this model and 10 is the total number of samples we have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "\n",
    "def get_train_batches(train_x, train_y, batch_size):\n",
    "    for i in range(0, train_x.shape[0], batch_size):    \n",
    "        yield train_x[i : i+batch_size], train_y[i : i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Given an input data of shape: [batch_size, seq_length], rnn_inputs returns a tensor of shape: [batch_size, seq_length, embedding_size]. This is purely driven by our decision to use an embedding matrix to represent each token (characters). Otherwise, we would've had to one-hot encode each character, and modify our input layer. I'm not sure what the complexity of this task would need to be, but using embedding matrices seem to be the preferred approach in most NLP applications of neural network. I think it massively improves the underlying computational complexity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of unique characters.. you'd normally do this by inspecting the data directly\n",
    "num_classes = 49 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(initial_value=tf.random_normal(mean=0., stddev=0.1, shape=[num_classes,embedding_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, ph_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_inputs:  [None, None, 18]\n"
     ]
    }
   ],
   "source": [
    "print(\"rnn_inputs: \",rnn_inputs.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main equation that powers LSTMs\n",
    "\n",
    "The equation below describes how cell-state at current time ($s_{t}$) must be updated as a function of the cell-state in the previous step ($s_{t-1}$), and input word at the current step ($x_t$). Note that the driving factors for the update are the factors: $i$, $f$, $o$, and most importantly, a hidden internal state $c_t$ in addition to the exposed internal weights $s_t$. The latter is considered exposed because it is also used to compute the outgoing logits, and hence the error vectors.\n",
    "\n",
    "#### [update the input (i), forget(f), and output(o) gates factors]\n",
    "\n",
    "$$ i = \\sigma ( x_t U^i + s_{t-1} W^i ) $$\n",
    "$$ f = \\sigma ( x_t U^f + s_{t-1} W^f ) $$\n",
    "$$ o = \\sigma ( x_t U^o + s_{t-1} W^o ) $$\n",
    "\n",
    "#### [update the hidden internal state: $c_t$ and the exposed internal state: $s_t$ \n",
    "\n",
    "$$ g = tanh ( x_t U^g + s_{t-1} W^g ) $$\n",
    "$$ c_t = c_{t-1} \\otimes f + g \\otimes i $$\n",
    "$$ s_t = tanh(c_t) \\otimes o $$\n",
    "\n",
    "Note that we're excluding the bias term for simplicity. Also the operator $\\otimes$ denotes an element-wise multiplication of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(previous, x):\n",
    "    '''\n",
    "    Method evaluates the new cell-state based on hidden\n",
    "    states from the previous step and input for given step.\n",
    "\n",
    "    \n",
    "    parameters:\n",
    "    - previous: This is the computed states from the\n",
    "    previous time_step. See the unrolled computational\n",
    "    graph in any vanilla LSTM tutorial.\n",
    "    \n",
    "    - x: input for current time_step    \n",
    "    '''\n",
    "    st_1, ct_1 = tf.unstack(previous)\n",
    "    \n",
    "    # Equations for the LSTM\n",
    "    i = tf.sigmoid(tf.matmul(x, U_i) + tf.matmul(st_1, W_i))\n",
    "    f = tf.sigmoid(tf.matmul(x, U_f) + tf.matmul(st_1, W_f))\n",
    "    o = tf.sigmoid(tf.matmul(x, U_o) + tf.matmul(st_1, W_o))\n",
    "    g = tf.sigmoid(tf.matmul(x, U_g) + tf.matmul(st_1, W_g))    \n",
    "    \n",
    "    ct = ct_1*f + g*i\n",
    "    st = tf.tanh(ct)*o\n",
    "    \n",
    "    return tf.stack([st,ct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.scan\n",
    "\n",
    "Method builds a loop that dynamically unforlds, to recursively apply the function step over all elements of the rnn_inputs. This is also one of the most important step to intuitively understand. I think without using the step function, it'd be extremely complex to implement the recursion successfully (and/or accurately)\n",
    "\n",
    "The dimensions also need to be reshuffled, so that the sequence length dimension is exposed as the 0th dimension. This is to enable iteration over elements of the sequence. I.e. Tensor of form [batch_size, seq_length, embedding_size] is transposed to [seq_length, batch_size, embedding_size]. The reshuffled tensor is represented by *transposed_inputs*\n",
    "\n",
    "The _states_ returned by scan is an array of states from all the time steps, using which we will predict the output probabilities at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transposed_inputs = tf.transpose(rnn_inputs,[1,0,2])\n",
    "\n",
    "'''\n",
    "tf.scan applies the step method recursively on the second argument, i.e\n",
    "transposed_inputs. The returned value from one execution will also be the input \n",
    "to the next call of the method. \n",
    "'''\n",
    "states = tf.scan(step, # the method that should be applied to each vec from below. Updates weights\n",
    "                 transposed_inputs, # [vec1, vec2, .... , vecN]: each vec. corresponds to the embedding row for a word.\n",
    "                                    # There are seqlen num. of words for every single batch. Hence, this method will recurse throug\n",
    "                                    # seqlen * batch_size times, each time returning a vector of length: state_size\n",
    "                 initializer=tf.stack([ph_init_state, ph_init_state])) # initializer. For computing the first argument (i.e. vec1), begin by a random set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states:  [None, 2, None, 37]\n"
     ]
    }
   ],
   "source": [
    "# [seq_len, state_type, batch_size, state_size]\n",
    "# where: state_type = s_t and c_t where the former (see 'step' function)\n",
    "# s_t: exposed state that will be used to evaluate logits in relation with\n",
    "#      the output layer, and\n",
    "# c_t: internal state that is used solely for updating LSTM cell's weights\n",
    "\n",
    "print(\"states: \",states.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The *states* tensor returned by _scan_ is of shape: [seqlen * batch-size, state_size]. Thus reshape to allow vector multiplication against the output network involving V and bo, where:\n",
    "\n",
    "- V: weight matrices for connecting cell-state to the output (or target)\n",
    "- b: bias vector for connecting cell-state to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define weights for the output layer\n",
    "V = tf.Variable(initial_value= tf.random_normal(mean=0., stddev=0.1, shape=[state_size, num_classes]))\n",
    "bo = tf.Variable(initial_value= tf.random_normal(mean=0., stddev=0.1, shape=[num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The consequence of this is permuting the dimensions of states so we get:\n",
    "# [state_type, batch_size, seqlen, state_size]\n",
    "states = tf.transpose(states, [1,2,0,3])\n",
    "\n",
    "# And get the first state_type (i.e. only s_t) because we only\n",
    "# need that to evaluate that to get the logits\n",
    "states = states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now finally reshape to get [seqlen * batch_size, state_size]\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(states_reshaped, V) + bo\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "ph_ys is shaped as follows: [batch_size, seq_length]. Look here to see what is happening in cell below. https://github.com/apiltamang/tensorflow_rtp_materials/blob/master/week-7/Vanilla_RNN_Using_Embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_ys_reshaped = tf.reshape(ph_ys, shape=[-1])\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=ph_ys_reshaped, logits=logits)\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2 # Take 2 at a time for the given 10 total samples we have available.\n",
    "seq_length = 200 # each vector is fixed length of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss evaluated:  3.89412\n",
      "loss evaluated:  3.75693\n",
      "loss evaluated:  3.68188\n",
      "loss evaluated:  3.58471\n",
      "loss evaluated:  3.42063\n",
      "loss evaluated:  3.25519\n",
      "loss evaluated:  3.08319\n",
      "loss evaluated:  3.21168\n",
      "loss evaluated:  3.23559\n",
      "loss evaluated:  3.13661\n",
      "loss evaluated:  3.06134\n",
      "loss evaluated:  2.94235\n",
      "loss evaluated:  3.20458\n",
      "loss evaluated:  3.19737\n",
      "loss evaluated:  3.06679\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_loss = 0.\n",
    "    n_epochs = 3\n",
    "    for i in range(n_epochs):\n",
    "        for xs, ys in get_train_batches(train_x, train_y, batch_size=2):\n",
    "            _, train_loss_val = sess.run([train_op, loss], \n",
    "                                         feed_dict={ph_xs: xs, \n",
    "                                                    ph_ys: ys,\n",
    "                                                    ph_init_state: np.zeros([batch_size, state_size])})\n",
    "            print(\"loss evaluated: \",train_loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
