{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN\n",
    "### Overview:\n",
    "The code works! While that in by itself is a major accomplishment, I'm not sure that will get me fame! Regardless, the focus is on understanding the vanilla RNN by developing it from a scratch. Everything, except for one/two specialized tensorflow method, should be completely transparent at the vector-wise mathematics level.\n",
    "\n",
    "The generated text at the end seems reasonable enough. With the relatively small amount of training data available, including a single layer RNN, I think the architecture is far from being optimal. There are so many hyper-parameters to explore (state_size, embedding_size, batch_size, learning_rate, num_layers etc. etc.) that this could be a weeks' worth of efforts by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based mostly on code and tutorial at\n",
    "\n",
    "(Someday I only hope I'll be able to write posts as well as the below)\n",
    "- https://github.com/suriyadeepan/rnn-from-scratch/blob/master/vanilla.py\n",
    "\n",
    "- http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n",
    "\n",
    "Some other resources to understand, especially, the origin of LSTMs and word-embeddings\n",
    "- https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "- http://sebastianruder.com/word-embeddings-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "state_size = 37 # (parameter for the RNN cell; describes how wide the cell should be)\n",
    "embedding_size = 27 #(parameter for embedding layer; describes the size of embedding for each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutable variables for weights and biases\n",
    "- W: weight matrices  for the recurrent relationship between cell states\n",
    "- U: weight matrices  for the connection between input and cell state\n",
    "- b: bias parameter for input -> cell state connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))\n",
    "U = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))\n",
    "b = tf.get_variable('b', shape=[state_size], initializer=tf.constant_initializer(0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_xs = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_ys = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_init_state = tf.placeholder(shape=[None, state_size], dtype=tf.float32, name=\"initial_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "The data in the files were populated using the *Data_Provider* notebook. *train_x* and *train_y* are both arrays of shape: [10,200], where 200 is a fixed sequence length for this model and 10 is the total number of samples we have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "\n",
    "def get_train_batches(train_x, train_y, batch_size):\n",
    "    for i in range(0, train_x.shape[0], batch_size):    \n",
    "        yield train_x[i : i+batch_size], train_y[i : i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some essential utilites from Data_Provider.py notebook\n",
    "import pickle\n",
    "vocab_to_int = pickle.load(open('vocab_to_int.txt','rb'))\n",
    "int_to_vocab = pickle.load(open('int_to_vocab.txt','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Given an input data of shape: [batch_size, seq_length], rnn_inputs returns a tensor of shape: [batch_size, seq_length, embedding_size]. This is purely driven by our decision to use an embedding matrix to represent each token (characters). Otherwise, we would've had to one-hot encode each character, and modify our input layer. I'm not sure what the complexity of this task would need to be, but using embedding matrices seem to be the preferred approach in most NLP applications of neural network. I think it massively improves the underlying computational complexity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of unique characters.. you'd normally do this by inspecting the data directly\n",
    "num_classes = 83\n",
    "embeddings = tf.Variable(initial_value=tf.random_normal(mean=0., stddev=0.1, shape=[num_classes,embedding_size]))\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, ph_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main equation that powers RNN\n",
    "\n",
    "The equation below describes how cell-state at current time ($s_{t}$) must be updated as a function of the cell-state in the previous step ($s_{t-1}$), and input word at the current step ($x_t$):\n",
    "$$ s_t = tanh ( W \\times s_{t-1} + U \\times x_{t} ) $$\n",
    "\n",
    "Note that we're excluding the bias term for simplicity. Also, the weight matrices(W and U) are universally shared, as is the matrix V, which is introduced later to evaluate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(hidden_previous, x):\n",
    "    '''\n",
    "    Method evaluates the new cell-state based on hidden\n",
    "    state from previous step, and input for given shape.\n",
    "    It uses the global shared parameter: W, U, and b.\n",
    "    \n",
    "    parameters:\n",
    "    - hidden_previous: This is the hidden state from the\n",
    "    previous time_step. See the unrolled computational\n",
    "    graph in any vanilla RNN tutorial.\n",
    "    \n",
    "    - x: input for current time_step    \n",
    "    '''\n",
    "    temp_a = tf.matmul(hidden_previous, W)\n",
    "    temp_b = tf.matmul(x, U) + b\n",
    "    return tf.tanh(temp_a + temp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.scan\n",
    "\n",
    "Method builds a loop that dynamically unforlds, to recursively apply the function step over all elements of the rnn_inputs. This is also one of the most important step to intuitively understand. I think without using the step function, it'd be extremely complex to implement the recursion successfully (and/or accurately)\n",
    "\n",
    "The dimensions also need to be reshuffled, so that the sequence length dimension is exposed as the 0th dimension. This is to enable iteration over elements of the sequence. I.e. Tensor of form [batch_size, seq_length, embedding_size] is transposed to [seq_length, batch_size, embedding_size]. The reshuffled tensor is represented by *transposed_inputs*\n",
    "\n",
    "The _states_ returned by scan is an array of states from all the time steps, using which we will predict the output probabilities at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transposed_inputs = tf.transpose(rnn_inputs,[1,0,2])\n",
    "\n",
    "'''\n",
    "tf.scan applies the step method recursively on the second argument, i.e\n",
    "transposed_inputs. The returned value from one execution will also be the input \n",
    "to the next call of the method. \n",
    "'''\n",
    "states = tf.scan(step, # the method that should be applied to each vec from below. Updates weights\n",
    "                 transposed_inputs, # [vec1, vec2, .... , vecN]: each vec. corresponds to the embedding row for a word.\n",
    "                                    # There are seqlen num. of words for every single batch. Hence, this method will recurse throug\n",
    "                                    # seqlen * batch_size times, each time returning a vector of length: state_size\n",
    "                 initializer=ph_init_state) # initializer. For computing the first argument (i.e. vec1), begin by a random set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 37]\n"
     ]
    }
   ],
   "source": [
    "# The shape below is [seqlen, batch_size, state_size]. It's basically a tensor where each character in each \n",
    "# sequence and batch has been encoded in the internal state of the RNN\n",
    "print (states.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that states returns a tensor with shape [seqlen, batch_size, state_size]. Getting states[-1] returns the last element along the first dimension, i.e. the tensor [batch_size, state_size] for the last seq. length (in other words, the last time-step). Assign it to *last_state*. This variable will eventually be used for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_state = states[-1]\n",
    "\n",
    "# transpose to: [batch_size, seqlen, state_size]\n",
    "states = tf.transpose(states,[1,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The *states* tensor returned by _scan_ is of shape: [seqlen * batch-size, state_size]. Thus reshape to allow vector multiplication against the output network involving V and bo, where:\n",
    "\n",
    "- V: weight matrices for connecting cell-state to the output (or target)\n",
    "- b: bias vector for connecting cell-state to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = tf.Variable(initial_value=tf.random_normal(mean=0., stddev=0.1, shape=[state_size, num_classes]))\n",
    "bo = tf.get_variable('bo', shape=[num_classes], initializer=tf.constant_initializer(0.))\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "logits = tf.matmul(states_reshaped, V) + bo\n",
    "\n",
    "# The shape of predictions will be: [seqlen * batch_size, num_classes]\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "ph_ys is shaped as follows: [batch_size, seq_length]. In order to use it against logits obtained by multiplying hidden_states (states) and output matrix (V), we need to collapse it into a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basically flatted ys into a flat vector\n",
    "ph_ys_reshaped = tf.reshape(ph_ys, shape=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow method: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "is very interesting. It (the method) will basically look at (the integer representation of) each character and evaluate a one-hot representation based on its label. Thus, ph_ys_reshaped, of size [batch_size * seqlen] will effectively be unrolled into a vector of length [batch_size * seqlen * num_classes]. The method then uses this unrolled vector to evaluate the loss against *logits*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=ph_ys_reshaped, logits=logits)\n",
    "loss = tf.reduce_mean(losses)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  0 , loss evaluated:  2.55229\n",
      "epochs:  1 , loss evaluated:  2.36326\n",
      "epochs:  2 , loss evaluated:  2.26657\n",
      "epochs:  3 , loss evaluated:  2.18202\n",
      "epochs:  4 , loss evaluated:  2.12148\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 # Take 100 at a time. We have 10,000 needed, so 100 iterations per epochs\n",
    "seq_length = 100 # each vector is fixed length of 100 (by inspecting data)\n",
    "chkpt_path = \"ckpts/\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_loss = 0.\n",
    "    n_epochs = 5\n",
    "    for i in range(n_epochs):\n",
    "        for xs, ys in get_train_batches(train_x, train_y, batch_size=100):\n",
    "            _, train_loss_val = sess.run([train_op, loss], \n",
    "                                         feed_dict={ph_xs: xs, \n",
    "                                                    ph_ys: ys,\n",
    "                                                    ph_init_state: np.zeros([batch_size, state_size])})\n",
    "        print(\"epochs: \",i, \", loss evaluated: \",train_loss_val)\n",
    "\n",
    "    # Save the model at the end of the run.\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, chkpt_path+\"vanilla_rnn.ckpt\", global_step=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restore_session(sess):\n",
    "    ckpt = tf.train.get_checkpoint_state(chkpt_path)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(\"restoring model from \",ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printChars(chars):\n",
    "    print('------- Generated Text ----------')\n",
    "    print(''.join(str(c) for c in chars))\n",
    "    print('-------      END        ---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring model from  ckpts/vanilla_rnn.ckpt-5\n",
      "------- Generated Text ----------\n",
      "21ing nike sold her tit\n",
      "to his toon of the croviteable as not aneve the lore, dasturcot-at the Vamady \n",
      "-------      END        ---------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess = restore_session(sess)\n",
    "    \n",
    "    current_char = vocab_to_int['g']\n",
    "    num_chars = 100             # number of char sequence to try generate\n",
    "    chars = [current_char]     # some input char to get started with text generation\n",
    "    batch_size = 1             # generate 1 batch a time (i wonder if its possible to specify a larger batch)\n",
    "    state = np.zeros([batch_size, state_size]) # initial state to start off with\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        preds, state = sess.run([predictions, last_state], \n",
    "                                feed_dict={ph_xs: np.array(current_char).reshape([1,1]),\n",
    "                                           ph_init_state: state})\n",
    "        current_char = np.random.choice(preds.shape[-1], 1, p=np.squeeze(preds))[0]\n",
    "        chars.append(int_to_vocab[current_char])\n",
    "    printChars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
