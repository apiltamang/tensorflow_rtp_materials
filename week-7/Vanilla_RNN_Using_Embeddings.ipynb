{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based mostly on code and tutorial at\n",
    "\n",
    "(Someday I only hope I'll be able to write posts as well as the below)\n",
    "- https://github.com/suriyadeepan/rnn-from-scratch/blob/master/vanilla.py\n",
    "\n",
    "- http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n",
    "\n",
    "Some other resources to understand, especially, the origin of LSTMs and word-embeddings\n",
    "- https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\n",
    "- http://sebastianruder.com/word-embeddings-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_size = 50 # (parameter for the RNN cell; describes how wide the cell should be)\n",
    "embedding_size = 25 #(parameter for embedding layer; describes the size of embedding for each token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutable variables for weights and biases\n",
    "- W: weight matrices  for the recurrent relationship between cell states\n",
    "- U: weight matrices  for the connection between input and cell state\n",
    "- b: bias parameter for input -> cell state connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[state_size, state_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = tf.Variable(initial_value=tf.random_normal(mean=0.,stddev=0.1,shape=[embedding_size, state_size]))\n",
    "b = tf.get_variable('b', shape=[state_size], initializer=tf.constant_initializer(0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_xs = tf.placeholder(shape=[None, None], dtype=tf.int32)\n",
    "ph_ys = tf.placeholder(shape=[None, None], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ph_init_state = tf.placeholder(shape=[None, state_size], dtype=tf.float32, name=\"initial_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "The data in the files were populated using the *Data_Provider* notebook. *train_x* and *train_y* are both arrays of shape: [10,200], where 200 is a fixed sequence length for this model and 10 is the total number of samples we have available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "\n",
    "def get_train_batches(train_x, train_y, batch_size):\n",
    "    for i in range(0, train_x.shape[0], batch_size):    \n",
    "        yield train_x[i : i+batch_size], train_y[i : i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Given an input data of shape: [batch_size, seq_length], rnn_inputs returns a tensor of shape: [batch_size, seq_length, embedding_size]. This is purely driven by our decision to use an embedding matrix to represent each token (characters). Otherwise, we would've had to one-hot encode each character, and modify our input layer. I'm not sure what the complexity of this task would need to be, but using embedding matrices seem to be the preferred approach in most NLP applications of neural network. I think it massively improves the underlying computational complexity as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of unique characters.. you'd normally do this by inspecting the data directly\n",
    "num_classes = 49 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(initial_value=tf.random_normal(mean=0., stddev=0.1, shape=[num_classes,embedding_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, ph_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main equation that powers RNN\n",
    "\n",
    "The equation below describes how cell-state at current time ($s_{t}$) must be updated as a function of the cell-state in the previous step ($s_{t-1}$), and input word at the current step ($x_t$):\n",
    "$$ s_t = tanh ( W \\times s_{t-1} + U \\times x_{t} ) $$\n",
    "\n",
    "Note that we're excluding the bias term for simplicity. Also, the weight matrices(W and U) are universally shared, as is the matrix V, which is introduced later to evaluate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(hidden_previous, x):\n",
    "    '''\n",
    "    Method evaluates the new cell-state based on hidden\n",
    "    state from previous step, and input for given shape.\n",
    "    It uses the global shared parameter: W, U, and b.\n",
    "    \n",
    "    parameters:\n",
    "    - hidden_previous: This is the hidden state from the\n",
    "    previous time_step. See the unrolled computational\n",
    "    graph in any vanilla RNN tutorial.\n",
    "    \n",
    "    - x: input for current time_step    \n",
    "    '''\n",
    "    temp_a = tf.matmul(hidden_previous, W)\n",
    "    temp_b = tf.matmul(x, U) + b\n",
    "    return tf.tanh(temp_a + temp_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.scan\n",
    "\n",
    "Method builds a loop that dynamically unforlds, to recursively apply the function step over all elements of the rnn_inputs. This is also one of the most important step to intuitively understand. I think without using the step function, it'd be extremely complex to implement the recursion successfully (and/or accurately)\n",
    "\n",
    "The dimensions also need to be reshuffled, so that the sequence length dimension is exposed as the 0th dimension. This is to enable iteration over elements of the sequence. I.e. Tensor of form [batch_size, seq_length, embedding_size] is transposed to [seq_length, batch_size, embedding_size]. The reshuffled tensor is represented by *transposed_inputs*\n",
    "\n",
    "The _states_ returned by scan is an array of states from all the time steps, using which we will predict the output probabilities at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transposed_inputs = tf.transpose(rnn_inputs,[1,0,2])\n",
    "\n",
    "'''\n",
    "tf.scan applies the step method recursively on the second argument, i.e\n",
    "transposed_inputs. The returned value from one execution will also be the input \n",
    "to the next call of the method. \n",
    "'''\n",
    "states = tf.scan(step, # the method that should be applied to each vec from below. Updates weights\n",
    "                 transposed_inputs, # [vec1, vec2, .... , vecN]: each vec. corresponds to the embedding row for a word.\n",
    "                                    # There are seqlen num. of words for every single batch. Hence, this method will recurse throug\n",
    "                                    # seqlen * batch_size times, each time returning a vector of length: state_size\n",
    "                 initializer=ph_init_state) # initializer. For computing the first argument (i.e. vec1), begin by a random set of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The *states* tensor returned by _scan_ is of shape: [seqlen * batch-size, state_size]. Thus reshape to allow vector multiplication against the output network involving V and bo, where:\n",
    "\n",
    "- V: weight matrices for connecting cell-state to the output (or target)\n",
    "- b: bias vector for connecting cell-state to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = tf.get_variable('V', shape=[state_size, num_classes], initializer=xav_init())\n",
    "\n",
    "bo = tf.get_variable('bo', shape=[num_classes], initializer=tf.constant_initializer(0.))\n",
    "\n",
    "states_reshaped = tf.reshape(states, [-1, state_size])\n",
    "\n",
    "logits = tf.matmul(states_reshaped, V) + bo\n",
    "\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "ph_ys is shaped as follows: [batch_size, seq_length]. In order to use it against logits obtained by multiplying hidden_states (states) and output matrix (V), we need to collapse it into a single dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_ys_reshaped = tf.reshape(ph_ys, shape=[-1])\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=ph_ys_reshaped, logits=logits)\n",
    "\n",
    "loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2 # Take 2 at a time for the given 10 total samples we have available.\n",
    "seq_length = 200 # each vector is fixed length of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss evaluated:  3.89611\n",
      "loss evaluated:  3.86409\n",
      "loss evaluated:  3.80358\n",
      "loss evaluated:  3.62748\n",
      "loss evaluated:  3.3962\n",
      "loss evaluated:  3.17762\n",
      "loss evaluated:  3.05978\n",
      "loss evaluated:  3.19444\n",
      "loss evaluated:  3.2264\n",
      "loss evaluated:  3.08712\n",
      "loss evaluated:  3.04819\n",
      "loss evaluated:  2.92335\n",
      "loss evaluated:  3.1668\n",
      "loss evaluated:  3.19675\n",
      "loss evaluated:  3.0758\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_loss = 0.\n",
    "    n_epochs = 3\n",
    "    for i in range(n_epochs):\n",
    "        for xs, ys in get_train_batches(train_x, train_y, batch_size=2):\n",
    "            _, train_loss_val = sess.run([train_op, loss], \n",
    "                                         feed_dict={ph_xs: xs, \n",
    "                                                    ph_ys: ys,\n",
    "                                                    ph_init_state: np.zeros([batch_size, state_size])})\n",
    "            print(\"loss evaluated: \",train_loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
