{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation using a Sequence-to-Sequence encoder-decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the data populated using the Data_Provider.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **source_int_text**: Represents the source text (in English), with each word mapped to a unique int ID\n",
    "- **target_int_text**: Represents the source text (in French), where each word is also mapped to a unique int ID\n",
    "\n",
    "**source_vocab_to_int** and **target_vocab_to_int** is a map that defines what unique ID each word in the source and target corpus is mapped to. **Target_int_to_vocab** and **Source_int_to_vocab** likewise, is the complimenting map that defines the mapping of words to IDs for the source and target corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source seq len:  17\n",
      "target seq len:  24\n"
     ]
    }
   ],
   "source": [
    "batch_size = 300\n",
    "max_source_seq_len = max([len(sentence) for sentence in source_int_text])\n",
    "max_target_seq_len = max([len(sentence) for sentence in target_int_text])\n",
    "print(\"source seq len: \", max_source_seq_len)\n",
    "print(\"target seq len: \", max_target_seq_len)\n",
    "\n",
    "source_seq_len_ = tf.placeholder_with_default(max_source_seq_len, None)\n",
    "target_seq_len_ = tf.placeholder_with_default(max_target_seq_len, None)\n",
    "\n",
    "inputs_ = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "targets_ = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "\n",
    "lr_ = tf.placeholder(tf.float32,name=\"learn_rate\")\n",
    "dropout_ = tf.placeholder(tf.float32, name=\"dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing target input\n",
    "\n",
    "We transform the target by\n",
    "- removing the last word from each sentence\n",
    "- placing the `<GO>` ID to the beginning of each sentence\n",
    "\n",
    "Also, note that we initially appended each target sentence with a `<EOS>` token. Removing the last word, thus, would only have the effect of getting rid of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    begin_indice = [0, 0]\n",
    "    end_indice = [batch_size, -1]\n",
    "    stride = [1, 1]\n",
    "    strided_slices = tf.strided_slice(target_data, begin_indice, end_indice, stride)\n",
    "    target_tensor_rank = [batch_size, 1]\n",
    "    value_to_fill_with = target_vocab_to_int['<GO>']\n",
    "    dummy_filled_tensor = tf.fill(target_tensor_rank, value_to_fill_with)\n",
    "    \n",
    "    dec_input = tf.concat( [dummy_filled_tensor, strided_slices], 1)\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_input_got: \n",
      " [[-99  10  20  30]\n",
      " [-99  40  18  23]\n",
      " [-99  12  14  15]\n",
      " [-99  33  44  55]\n",
      " [-99  11  22  33]]\n",
      "done with my little experiment\n"
     ]
    }
   ],
   "source": [
    "# #My own little experiment to understand the process_decoding_input method:\n",
    "\n",
    "def test_process_decoding_input(process_decoding_input):\n",
    "    ''' \n",
    "    Method is used to modify the input to the decoder.\n",
    "    The input to the decoder is the target translated language\n",
    "    '''\n",
    "\n",
    "    batch_size = 5\n",
    "    seq_length = 9\n",
    "    target_vocab_to_int = {'<GO>': -99}\n",
    "    with tf.Graph().as_default():\n",
    "        target_data = tf.placeholder(tf.int32, [None, None])\n",
    "        dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "\n",
    "        test_target_data = [[10, 20, 30, 99], [40, 18, 23,23],[12,14,15,33],[33,44,55,87],[11,22,33,78]]\n",
    "        with tf.Session() as sess:\n",
    "            test_dec_input = sess.run(dec_input, {target_data: test_target_data})\n",
    "       \n",
    "        print(\"dec_input_got: \\n\",test_dec_input)\n",
    "        print (\"done with my little experiment\")\n",
    "\n",
    "test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Apply embedding to input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc embed input:  [None, None, 255]\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(source_vocab_to_int)\n",
    "enc_embed_size = 255\n",
    "enc_embed_input = tf.contrib.layers.embed_sequence(inputs_, source_vocab_size, enc_embed_size)\n",
    "\n",
    "# shape: (batch_size, seq_length, embedding_size)\n",
    "print(\"enc embed input: \", enc_embed_input.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement the \"encoder\" layer of the encoder-decoder architecture.\n",
    "This is just going to be a regular LSTM cell with some dropout. Also, we'll be using the embedding from above to feed the input to this unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 128\n",
    "num_layers = 2\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.5)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of enc_state is [batch_size, cell.state_size (=64 in this case)]\n",
    "_ , enc_cell_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the \"decoder\" layer of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Get the input to the decoder by running it through the decoding_input_processor.\n",
    "Remember that this is simply preprocessing, that could (and should) probably have been accomplished through pure numpy functions. The usage of the _striated_stride_, _fill_, and _concat_ operators thoroughly confuse me, especially not being able to see explicitly what is happening to the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, None]\n"
     ]
    }
   ],
   "source": [
    "dec_input = process_decoding_input(target_data=targets_, target_vocab_to_int=target_vocab_to_int, batch_size=batch_size)\n",
    "# shape: [batch_size, seq_length]\n",
    "print(dec_input.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### b. Embed the input to the decoding sequence (just like the input to the encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = len(target_vocab_to_int)\n",
    "dec_embed_size = 225\n",
    "dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, dec_embed_size]))\n",
    "dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Define the decoder RNN Cell and training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size_dec = 128\n",
    "num_layers = 2\n",
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size_dec)\n",
    "drop_dec = tf.contrib.rnn.DropoutWrapper(lstm_dec, output_keep_prob=0.5)\n",
    "dec_cell = tf.contrib.rnn.MultiRNNCell([drop_dec] * num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  c. Decoder training function\n",
    "This is really weird, and a symptom of tensorflow's API being too low-level (or just plain confusing). Anyways, doing what needs to be done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state=enc_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Constructing the dynamic_rnn_decoder\n",
    "This method is listed as being comparable to the 'dynamic_rnn' method that I've used several times before for the dynamic unrolling of RNNs. Doc at: https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder. Method seems\n",
    "to require a 'decoder_fn', which is provided by the invocation of the tf.contrib.seq2seq.simple_decoder_fn_train(..) method call.\n",
    "\n",
    "The most important output at the end of this is the var: **train_logits**. We will be using **train_logits** to evaluate the loss and ultimately run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fn = lambda x: tf.contrib.layers.fully_connected(x, target_vocab_size, None, scope=decoding_scope)\n",
    "\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    train_pred, _ , _ = tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell, decoder_fn=train_decoder_fn, inputs=dec_embed_input, sequence_length=target_seq_len_, scope=decoding_scope)\n",
    "    train_logits = output_fn(train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Decoding inference\n",
    "The most important value at the end of this setup is: **infer_logits**. The infer_logits is the node that will be used for getting model outputs (i.e get translations from the model). These outputs can further be used to evaluate validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sequence_id = target_vocab_to_int['<GO>']\n",
    "end_sequence_id = target_vocab_to_int['<EOS>']\n",
    "\n",
    "infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn, enc_cell_state, dec_embeddings, start_sequence_id, end_sequence_id, target_seq_len_, target_vocab_size)\n",
    "\n",
    "with tf.variable_scope(\"decoding\", reuse=True) as decoding_scope:\n",
    "    infer_logits, _, _= tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell, decoder_fn=infer_decoder_fn, inputs=None, sequence_length=target_seq_len_,scope=decoding_scope)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Specify the loss and optimizer methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logits:0' shape=(?, ?, 358) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just give the tensor a name.. After the model is saved and loaded back, we can then get this tensor by its name\n",
    "# i.e. 'logits:0' which can be used to form translations for us. \n",
    "tf.identity(infer_logits, 'logits') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(train_logits, targets_, tf.ones([batch_size, target_seq_len_]))\n",
    "    optimizer = tf.train.AdamOptimizer(lr_)\n",
    "    gradients = optimizer.compute_gradients(cost)    \n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Train_logits is conditioned on the **encoder state** (which accepts input from the _source_ language), and also accepts input also from the **targets** (i.e. the _target_ language). Thus it's reasonable to assume that tf.contrib.seq2seq.sequence_loss legitimately evaluates the cost.\n",
    "\n",
    "Also, from the documentation of tf.contrib.seq2seq.sequence_loss, the last parameter (weights) constitutes the weighting of each prediction in the sequence. I am not sure I understand this completely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_logits:  [300, None, 358]\n",
      "targets:  [None, None]\n",
      "weights:  [300, <tf.Tensor 'PlaceholderWithDefault_1:0' shape=<unknown> dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_logits: \",train_logits.shape.as_list())\n",
    "print(\"targets: \", targets_.shape.as_list())\n",
    "print(\"weights: \", [batch_size, target_seq_len_]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = helper.batch_data(source_int_text, target_int_text, batch_size)\n",
    "num_batch_per_epoch = len(source_int_text) // batch_size\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = helper.pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = helper.pad_sentence_batch(target_int_text[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_epochs = 0\n",
    "end_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Train Acc:  0.518 Valid Acc:  0.517 Loss:  1.458\n",
      "Epoch   0 Train Acc:  0.566 Valid Acc:  0.503 Loss:  0.901\n",
      "Epoch   0 Train Acc:  0.537 Valid Acc:  0.594 Loss:  0.747\n",
      "Epoch   0 Train Acc:  0.544 Valid Acc:  0.545 Loss:  0.747\n",
      "Epoch   1 Train Acc:  0.579 Valid Acc:  0.623 Loss:  0.668\n",
      "Epoch   1 Train Acc:  0.586 Valid Acc:  0.588 Loss:  0.626\n",
      "Epoch   1 Train Acc:  0.636 Valid Acc:  0.632 Loss:  0.551\n",
      "Epoch   1 Train Acc:  0.632 Valid Acc:  0.627 Loss:  0.551\n",
      "Epoch   2 Train Acc:  0.647 Valid Acc:  0.660 Loss:  0.492\n",
      "Epoch   2 Train Acc:  0.678 Valid Acc:  0.671 Loss:  0.428\n",
      "Epoch   2 Train Acc:  0.709 Valid Acc:  0.741 Loss:  0.400\n",
      "Epoch   2 Train Acc:  0.714 Valid Acc:  0.687 Loss:  0.400\n",
      "Epoch   3 Train Acc:  0.730 Valid Acc:  0.738 Loss:  0.357\n",
      "Epoch   3 Train Acc:  0.768 Valid Acc:  0.765 Loss:  0.303\n",
      "Epoch   3 Train Acc:  0.788 Valid Acc:  0.777 Loss:  0.281\n",
      "Epoch   3 Train Acc:  0.788 Valid Acc:  0.762 Loss:  0.281\n",
      "Epoch   4 Train Acc:  0.780 Valid Acc:  0.806 Loss:  0.256\n",
      "Epoch   4 Train Acc:  0.821 Valid Acc:  0.808 Loss:  0.219\n",
      "Epoch   4 Train Acc:  0.821 Valid Acc:  0.820 Loss:  0.206\n",
      "Epoch   4 Train Acc:  0.849 Valid Acc:  0.835 Loss:  0.206\n",
      "Epoch   5 Train Acc:  0.819 Valid Acc:  0.817 Loss:  0.197\n",
      "Epoch   5 Train Acc:  0.822 Valid Acc:  0.840 Loss:  0.167\n",
      "Epoch   5 Train Acc:  0.851 Valid Acc:  0.868 Loss:  0.161\n",
      "Epoch   5 Train Acc:  0.861 Valid Acc:  0.843 Loss:  0.161\n",
      "Epoch   6 Train Acc:  0.848 Valid Acc:  0.877 Loss:  0.149\n",
      "Epoch   6 Train Acc:  0.908 Valid Acc:  0.851 Loss:  0.121\n",
      "Epoch   6 Train Acc:  0.875 Valid Acc:  0.884 Loss:  0.131\n",
      "Epoch   6 Train Acc:  0.886 Valid Acc:  0.891 Loss:  0.131\n",
      "Epoch   7 Train Acc:  0.890 Valid Acc:  0.878 Loss:  0.124\n",
      "Epoch   7 Train Acc:  0.893 Valid Acc:  0.878 Loss:  0.102\n",
      "Epoch   7 Train Acc:  0.895 Valid Acc:  0.894 Loss:  0.104\n",
      "Epoch   7 Train Acc:  0.881 Valid Acc:  0.884 Loss:  0.104\n",
      "Epoch   8 Train Acc:  0.881 Valid Acc:  0.891 Loss:  0.111\n",
      "Epoch   8 Train Acc:  0.922 Valid Acc:  0.894 Loss:  0.077\n",
      "Epoch   8 Train Acc:  0.910 Valid Acc:  0.908 Loss:  0.087\n",
      "Epoch   8 Train Acc:  0.912 Valid Acc:  0.895 Loss:  0.087\n",
      "Epoch   9 Train Acc:  0.888 Valid Acc:  0.913 Loss:  0.094\n",
      "Epoch   9 Train Acc:  0.913 Valid Acc:  0.905 Loss:  0.069\n",
      "Epoch   9 Train Acc:  0.906 Valid Acc:  0.912 Loss:  0.082\n",
      "Epoch   9 Train Acc:  0.920 Valid Acc:  0.902 Loss:  0.082\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(start_epochs, end_epochs):\n",
    "    batch_data = helper.batch_data(train_source, train_target, batch_size)\n",
    "    cntr = 0\n",
    "    for batch_i, (source_batch, target_batch) in enumerate(batch_data):\n",
    "        if(batch_i < batch_size):\n",
    "            # ------- compute loss -----------------\n",
    "            _ , loss = sess.run([train_op, cost], {\n",
    "                inputs_ : source_batch,\n",
    "                targets_ : target_batch,\n",
    "                source_seq_len_ : source_batch.shape[1],\n",
    "                target_seq_len_ : target_batch.shape[1],\n",
    "                lr_ : 0.005,\n",
    "                dropout_ : 0.5\n",
    "            })\n",
    "\n",
    "        batch_train_logits = sess.run(\n",
    "            infer_logits,\n",
    "            {inputs_: source_batch, dropout_: 1.0})\n",
    "        batch_valid_logits = sess.run(\n",
    "            infer_logits,\n",
    "            {inputs_: valid_source, dropout_: 1.0})\n",
    "\n",
    "        train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "        valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "\n",
    "        # ------- print stats ------------------ \n",
    "        cntr+=1\n",
    "        if(cntr%100==0):                \n",
    "            print('Epoch {:>3} Train Acc: {:>6.3f} Valid Acc: {:>6.3f} Loss: {:>6.3f}'.format(epoch_i, train_acc, valid_acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saved_models/models'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------- save the model from the last epoch --------- \n",
    "saver.save(sess,\"./saved_models/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Inference and Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the model all set-up, have a little fun with the translation. Of course, you should consider training the model for much longer, or use a more exhaustive data corpus. Think there is a bigger one (English->French) which is many millions of lines long (ours is ~130,000 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ids = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        word_id = -99\n",
    "        \n",
    "        if word in vocab_to_int:\n",
    "            word_id = vocab_to_int[word]\n",
    "        else:\n",
    "            word_id = vocab_to_int['<UNK>']\n",
    "\n",
    "        ids.append(word_id)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_french(english_sentence):\n",
    "\n",
    "    translate_sentence = sentence_to_seq(english_sentence, source_vocab_to_int)\n",
    "    load_path = \"./saved_models/models\"\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "        loader.restore(sess, load_path)\n",
    "\n",
    "        # inputs_ and dropout_ were identified by the names: input and dropout.\n",
    "        # That's why we can use the below..\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('dropout:0')\n",
    "\n",
    "        # tf.identity(infer_logits,\"logits\") establishes this relationship\n",
    "        logits = loaded_graph.get_tensor_by_name('logits:0')     \n",
    "\n",
    "        translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "        \n",
    "        translate_text = 'In French: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)])\n",
    "        \n",
    "        return translate_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Very bad indeed.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In French: [\"j\\'aime\", \"l\\'\", \\'aimée\\', \\'.\\', \\'<EOS>\\']'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_french(\"I like yellow apples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In French: ['nos', 'est', 'moins', 'été', '.', '<EOS>']\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_french(\"where is your man?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In French: ['comment', ',', 'de', 'mangues', 'et', 'les', 'à', 'sec', '.', '<EOS>']\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_french(\"I love yellow trucks and cars.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
