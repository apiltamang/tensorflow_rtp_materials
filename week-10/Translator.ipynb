{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation using a Sequence-to-Sequence encoder-decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the data populated using the Data_Provider.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source seq len:  17\n",
      "target seq len:  22\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_source_seq_len = max([len(sentence) for sentence in source_int_text])\n",
    "max_target_seq_len = max([len(sentence) for sentence in target_int_text])\n",
    "print(\"source seq len: \", max_source_seq_len)\n",
    "print(\"target seq len: \", max_target_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_seq_len_ = tf.placeholder_with_default(max_source_seq_len, None)\n",
    "target_seq_len_ = tf.placeholder_with_default(max_target_seq_len, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "targets_ = tf.placeholder(tf.int32, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_ = tf.placeholder(tf.float32)\n",
    "dropout_ = tf.placeholder(tf.float32, name=\"dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "print(len(source_int_text))\n",
    "print(len(target_int_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing target input\n",
    "\n",
    "For some reason, unknown at the moment, we transform the target by\n",
    "- removing the last word from each sentence\n",
    "- placing the `<GO>` ID to the beginning of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    begin_indice = [0, 0]\n",
    "    end_indice = [batch_size, -1]\n",
    "    stride = [1, 1]\n",
    "    strided_slices = tf.strided_slice(target_data, begin_indice, end_indice, stride)\n",
    "    print(\"strided_slices: \", strided_slices)    \n",
    "    \n",
    "    target_tensor_rank = [batch_size, 1]\n",
    "    value_to_fill_with = target_vocab_to_int['<GO>']\n",
    "    dummy_filled_tensor = tf.fill(target_tensor_rank, value_to_fill_with)\n",
    "    \n",
    "    dec_input = tf.concat( [dummy_filled_tensor, strided_slices], 1)\n",
    "    print(\"dec_input: \",dec_input)    \n",
    "\n",
    "    return dec_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slices:  Tensor(\"StridedSlice:0\", shape=(?, ?), dtype=int32)\n",
      "dec_input:  Tensor(\"concat:0\", shape=(5, ?), dtype=int32)\n",
      "dec_input_got: \n",
      " [[-99  10  20  30]\n",
      " [-99  40  18  23]\n",
      " [-99  12  14  15]\n",
      " [-99  33  44  55]\n",
      " [-99  11  22  33]]\n",
      "done with my little experiment\n"
     ]
    }
   ],
   "source": [
    "# #My own little experiment to understand the process_decoding_input method:\n",
    "\n",
    "def test_process_decoding_input(process_decoding_input):\n",
    "    ''' \n",
    "    Method is used to modify the input to the decoder.\n",
    "    The input to the decoder is the target translated language\n",
    "    '''\n",
    "\n",
    "    batch_size = 5\n",
    "    seq_length = 9\n",
    "    target_vocab_to_int = {'<GO>': -99}\n",
    "    with tf.Graph().as_default():\n",
    "        target_data = tf.placeholder(tf.int32, [None, None])\n",
    "        dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "\n",
    "        test_target_data = [[10, 20, 30, 99], [40, 18, 23,23],[12,14,15,33],[33,44,55,87],[11,22,33,78]]\n",
    "        with tf.Session() as sess:\n",
    "            test_dec_input = sess.run(dec_input, {target_data: test_target_data})\n",
    "       \n",
    "        print(\"dec_input_got: \\n\",test_dec_input)\n",
    "        print (\"done with my little experiment\")\n",
    "\n",
    "test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Apply embedding to input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size 231\n"
     ]
    }
   ],
   "source": [
    "source_vocab_size = len(source_vocab_to_int)\n",
    "print(\"source vocab size\", source_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_embed_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shape: (batch_size, seq_length, embedding_size)\n",
    "enc_embed_input = tf.contrib.layers.embed_sequence(inputs_, source_vocab_size, enc_embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc embed input:  [None, None, 27]\n"
     ]
    }
   ],
   "source": [
    "print(\"enc embed input: \", enc_embed_input.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the \"encoder\" layer of the encoder-decoder architecture, using the embeddings from above.\n",
    "This is just going to be a regular LSTM cell with some dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 64\n",
    "num_layers = 1\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.5)\n",
    "enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of enc_state is [batch_size, cell.state_size (=64 in this case)]\n",
    "_ , enc_cell_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc cell state:  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>),)\n"
     ]
    }
   ],
   "source": [
    "print(\"enc cell state: \", enc_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the input to the decoder by running it through the decoding_input_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slices:  Tensor(\"StridedSlice:0\", shape=(?, ?), dtype=int32)\n",
      "dec_input:  Tensor(\"concat:0\", shape=(32, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dec_input = process_decoding_input(target_data=targets_, target_vocab_to_int=target_vocab_to_int, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, None]\n"
     ]
    }
   ],
   "source": [
    "# shape: [batch_size, seq_length]\n",
    "print(dec_input.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Embed the input to the decoding sequence (just like the input to the encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_vocab_size = len(target_vocab_to_int)\n",
    "dec_embed_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vocab size:  358\n"
     ]
    }
   ],
   "source": [
    "print(\"target vocab size: \",target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, dec_embed_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec embed input:  [32, None, 30]\n"
     ]
    }
   ],
   "source": [
    "print(\"dec embed input: \",dec_embed_input.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size_dec = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_dec = tf.contrib.rnn.BasicLSTMCell(lstm_size_dec)\n",
    "drop_dec = tf.contrib.rnn.DropoutWrapper(lstm, dropout_)\n",
    "dec_cell = tf.contrib.rnn.MultiRNNCell([drop_dec] * num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state=enc_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamic_rnn_decoder\n",
    "This method is listed as being comparable to the 'dynamic_rnn' method that I've used several times before for the dynamic unrolling of RNNs. Doc at: https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder. Method seems\n",
    "to require a 'decoder_fn', which is provided by the invocation of the tf.contrib.seq2seq.simple_decoder_fn_train(..) method call.\n",
    "\n",
    "The most important output at the end of this is the var: **train_logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_fn = lambda x: tf.contrib.layers.fully_connected(x, target_vocab_size, None, scope=decoding_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    train_pred, _ , _ = tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell, decoder_fn=train_decoder_fn, inputs=dec_embed_input, sequence_length=target_seq_len_, scope=decoding_scope)\n",
    "    train_logits = output_fn(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoding/Reshape_1:0\", shape=(32, ?, 358), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(train_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, None, 358]\n"
     ]
    }
   ],
   "source": [
    "print(train_logits.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding inference\n",
    "The most important value at the end of this setup is: **infer_logits**. The infer_logits is the node that will be used to do validation (i.e compute accuracy, form translation etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_sequence_id = target_vocab_to_int['<GO>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_sequence_id = target_vocab_to_int['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1\n"
     ]
    }
   ],
   "source": [
    "print (start_sequence_id, end_sequence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn, enc_cell_state, dec_embeddings, start_sequence_id, end_sequence_id, target_seq_len_, target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function simple_decoder_fn_inference.<locals>.decoder_fn at 0x1211a8268>\n"
     ]
    }
   ],
   "source": [
    "print(infer_decoder_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoding\", reuse=True) as decoding_scope:\n",
    "    infer_logits, _, _= tf.contrib.seq2seq.dynamic_rnn_decoder(cell=dec_cell, decoder_fn=infer_decoder_fn, inputs=None, sequence_length=target_seq_len_,scope=decoding_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 358]\n"
     ]
    }
   ],
   "source": [
    "print(infer_logits.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specify the loss and optimizer methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logits:0' shape=(?, ?, 358) dtype=float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just give the tensor a name.. After the model is saved and loaded back, we can then get this tensor by its name\n",
    "# i.e. 'logits:0' which can be used to form translations for us. \n",
    "tf.identity(infer_logits, 'logits') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(train_logits, targets_, tf.ones([batch_size, target_seq_len_]))\n",
    "    optimizer = tf.train.AdamOptimizer(lr_)\n",
    "    gradients = optimizer.compute_gradients(cost)    \n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Train_logits is conditioned on the **encoder state** (which accepts input from the _source_ language), and also accepts input also from the **targets** (i.e. the _target_ language). Thus it's reasonable to assume that tf.contrib.seq2seq.sequence_loss legitimately evaluates the cost.\n",
    "\n",
    "Also, from the documentation of tf.contrib.seq2seq.sequence_loss, the last parameter (weights) constitutes the weighting of each prediction in the sequence. I am not sure I understand this completely..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_logits:  [32, None, 358]\n",
      "targets:  [None, None]\n",
      "weights:  [32, <tf.Tensor 'PlaceholderWithDefault_1:0' shape=<unknown> dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_logits: \",train_logits.shape.as_list())\n",
    "print(\"targets: \", targets_.shape.as_list())\n",
    "print(\"weights: \", [batch_size, target_seq_len_]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = helper.batch_data(source_int_text, target_int_text, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch_per_epoch = len(source_int_text) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "print(num_batch_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "print(num_batch_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = helper.pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = helper.pad_sentence_batch(target_int_text[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Train Acc:  0.001 Valid Acc:  0.004 Loss:  5.900\n",
      "Epoch   0 Train Acc:  0.057 Valid Acc:  0.058 Loss:  5.850\n",
      "Epoch   0 Train Acc:  0.268 Valid Acc:  0.277 Loss:  5.828\n",
      "Epoch   0 Train Acc:  0.318 Valid Acc:  0.300 Loss:  5.789\n",
      "Epoch   0 Train Acc:  0.270 Valid Acc:  0.300 Loss:  5.731\n",
      "Epoch   0 Train Acc:  0.355 Valid Acc:  0.325 Loss:  5.701\n",
      "Epoch   0 Train Acc:  0.357 Valid Acc:  0.323 Loss:  5.650\n",
      "Epoch   0 Train Acc:  0.365 Valid Acc:  0.323 Loss:  5.630\n",
      "Epoch   0 Train Acc:  0.322 Valid Acc:  0.323 Loss:  5.622\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.323 Loss:  5.556\n",
      "Epoch   0 Train Acc:  0.307 Valid Acc:  0.323 Loss:  5.538\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.323 Loss:  5.427\n",
      "Epoch   0 Train Acc:  0.344 Valid Acc:  0.323 Loss:  5.386\n",
      "Epoch   0 Train Acc:  0.353 Valid Acc:  0.323 Loss:  5.319\n",
      "Epoch   0 Train Acc:  0.344 Valid Acc:  0.323 Loss:  5.278\n",
      "Epoch   0 Train Acc:  0.338 Valid Acc:  0.323 Loss:  5.266\n",
      "Epoch   0 Train Acc:  0.361 Valid Acc:  0.323 Loss:  5.179\n",
      "Epoch   0 Train Acc:  0.337 Valid Acc:  0.323 Loss:  5.018\n",
      "Epoch   0 Train Acc:  0.319 Valid Acc:  0.323 Loss:  5.108\n",
      "Epoch   0 Train Acc:  0.334 Valid Acc:  0.323 Loss:  4.958\n",
      "Epoch   0 Train Acc:  0.334 Valid Acc:  0.323 Loss:  4.884\n",
      "Epoch   0 Train Acc:  0.338 Valid Acc:  0.323 Loss:  4.710\n",
      "Epoch   0 Train Acc:  0.307 Valid Acc:  0.323 Loss:  4.762\n",
      "Epoch   0 Train Acc:  0.322 Valid Acc:  0.323 Loss:  4.590\n",
      "Epoch   0 Train Acc:  0.352 Valid Acc:  0.323 Loss:  4.629\n",
      "Epoch   0 Train Acc:  0.395 Valid Acc:  0.333 Loss:  4.496\n",
      "Epoch   0 Train Acc:  0.327 Valid Acc:  0.334 Loss:  4.407\n",
      "Epoch   0 Train Acc:  0.385 Valid Acc:  0.334 Loss:  4.319\n",
      "Epoch   0 Train Acc:  0.349 Valid Acc:  0.334 Loss:  4.294\n",
      "Epoch   0 Train Acc:  0.372 Valid Acc:  0.334 Loss:  4.192\n",
      "Epoch   0 Train Acc:  0.357 Valid Acc:  0.334 Loss:  4.061\n",
      "Epoch   0 Train Acc:  0.344 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.323 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.378 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.359 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.336 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.334 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.344 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.383 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.344 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.374 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.359 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.374 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.365 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.351 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.327 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.361 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.361 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.330 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.348 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.375 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.355 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.319 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.390 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.340 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.352 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.357 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.371 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.337 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.342 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.333 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.349 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.346 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.364 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.353 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.368 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.399 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.363 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.348 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.383 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.363 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.357 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.379 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.352 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.363 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.333 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.383 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.398 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.360 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.346 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.340 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.370 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.341 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.365 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.378 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.337 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.300 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.346 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.340 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.367 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.353 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.321 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.365 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.374 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.319 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.357 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.322 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.395 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.352 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.359 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.330 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.382 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.360 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.363 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.342 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.327 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.308 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.376 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.363 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.326 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.382 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.378 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.387 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.385 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.383 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.370 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.395 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.372 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.351 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.382 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.336 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.364 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.364 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.359 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.386 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.379 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.386 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.348 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.349 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.348 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.386 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.332 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.330 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.368 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.352 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.355 Valid Acc:  0.342 Loss:  4.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Train Acc:  0.371 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.304 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.394 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.370 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.367 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.370 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.398 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.349 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.345 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   0 Train Acc:  0.372 Valid Acc:  0.342 Loss:  4.179\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.351 Loss:  4.083\n",
      "Epoch   1 Train Acc:  0.340 Valid Acc:  0.352 Loss:  3.802\n",
      "Epoch   1 Train Acc:  0.404 Valid Acc:  0.352 Loss:  3.856\n",
      "Epoch   1 Train Acc:  0.360 Valid Acc:  0.352 Loss:  3.839\n",
      "Epoch   1 Train Acc:  0.376 Valid Acc:  0.352 Loss:  3.653\n",
      "Epoch   1 Train Acc:  0.378 Valid Acc:  0.352 Loss:  3.734\n",
      "Epoch   1 Train Acc:  0.386 Valid Acc:  0.352 Loss:  3.638\n",
      "Epoch   1 Train Acc:  0.393 Valid Acc:  0.352 Loss:  3.743\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.355 Loss:  3.750\n",
      "Epoch   1 Train Acc:  0.357 Valid Acc:  0.355 Loss:  3.795\n",
      "Epoch   1 Train Acc:  0.341 Valid Acc:  0.355 Loss:  3.854\n",
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.355 Loss:  3.655\n",
      "Epoch   1 Train Acc:  0.376 Valid Acc:  0.356 Loss:  3.692\n",
      "Epoch   1 Train Acc:  0.389 Valid Acc:  0.356 Loss:  3.650\n",
      "Epoch   1 Train Acc:  0.374 Valid Acc:  0.356 Loss:  3.652\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.780\n",
      "Epoch   1 Train Acc:  0.387 Valid Acc:  0.356 Loss:  3.816\n",
      "Epoch   1 Train Acc:  0.365 Valid Acc:  0.356 Loss:  3.490\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.795\n",
      "Epoch   1 Train Acc:  0.368 Valid Acc:  0.356 Loss:  3.652\n",
      "Epoch   1 Train Acc:  0.367 Valid Acc:  0.356 Loss:  3.606\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.448\n",
      "Epoch   1 Train Acc:  0.344 Valid Acc:  0.356 Loss:  3.604\n",
      "Epoch   1 Train Acc:  0.355 Valid Acc:  0.356 Loss:  3.456\n",
      "Epoch   1 Train Acc:  0.385 Valid Acc:  0.356 Loss:  3.593\n",
      "Epoch   1 Train Acc:  0.414 Valid Acc:  0.356 Loss:  3.635\n",
      "Epoch   1 Train Acc:  0.346 Valid Acc:  0.356 Loss:  3.494\n",
      "Epoch   1 Train Acc:  0.401 Valid Acc:  0.356 Loss:  3.531\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.487\n",
      "Epoch   1 Train Acc:  0.390 Valid Acc:  0.356 Loss:  3.449\n",
      "Epoch   1 Train Acc:  0.378 Valid Acc:  0.356 Loss:  3.392\n",
      "Epoch   1 Train Acc:  0.353 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.336 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.385 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.365 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.351 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.345 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.367 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.353 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.391 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.357 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.371 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.383 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.364 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.382 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.375 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.376 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.355 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.371 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.368 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.337 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.359 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.382 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.374 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.341 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.408 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.365 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.364 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.385 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.344 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.342 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.353 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.389 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.368 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.386 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.417 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.370 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.360 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.397 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.374 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.370 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.385 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.378 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.359 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.359 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.397 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.406 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.368 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.353 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.365 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.376 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.351 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.371 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.382 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.351 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.310 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.365 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.353 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.390 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.360 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.342 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.374 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.383 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.344 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.370 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.344 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.404 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.360 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.367 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.387 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.368 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.370 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.334 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.379 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.374 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.391 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.387 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.393 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.393 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.344 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.391 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.378 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.402 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.382 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.340 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.386 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.357 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.346 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.375 Valid Acc:  0.356 Loss:  3.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.387 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.352 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.367 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.398 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.389 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.404 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.394 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.351 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.361 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.380 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.312 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.404 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.389 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.372 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.390 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.406 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.356 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.355 Valid Acc:  0.356 Loss:  3.530\n",
      "Epoch   1 Train Acc:  0.386 Valid Acc:  0.356 Loss:  3.530\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        batch_data = helper.batch_data(train_source, target_int_text, batch_size)\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(batch_data):\n",
    "            if(batch_i < batch_size):\n",
    "                # ------- compute loss -----------------\n",
    "                _ , loss = sess.run([train_op, cost], {\n",
    "                    inputs_ : source_batch,\n",
    "                    targets_ : target_batch,\n",
    "                    source_seq_len_ : len(source_batch[batch_i]),\n",
    "                    target_seq_len_ : len(target_batch[batch_i]),\n",
    "                    lr_ : 0.001,\n",
    "                    dropout_ : 0.5\n",
    "                })\n",
    "\n",
    "            batch_train_logits = sess.run(\n",
    "                infer_logits,\n",
    "                {inputs_: source_batch, dropout_: 1.0})\n",
    "            batch_valid_logits = sess.run(\n",
    "                infer_logits,\n",
    "                {inputs_: valid_source, dropout_: 1.0})\n",
    "                \n",
    "            train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "            valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "                \n",
    "            # ------- print stats ------------------            \n",
    "            print('Epoch {:>3} Train Acc: {:>6.3f} Valid Acc: {:>6.3f} Loss: {:>6.3f}'.format(epoch_i, train_acc, valid_acc, loss))\n",
    "    \n",
    "    # ------- save the model from the last epoch --------- \n",
    "    saver.save(sess,\"./saved_models/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ids = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        word_id = -99\n",
    "        \n",
    "        if word in vocab_to_int:\n",
    "            word_id = vocab_to_int[word]\n",
    "        else:\n",
    "            word_id = vocab_to_int['<UNK>']\n",
    "\n",
    "        ids.append(word_id)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_path = \"./saved_models/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [163, 99, 96, 225, 134, 50, 180]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [52, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: ['est', 'est', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    # inputs_ and dropout_ were identified by the names: input and dropout.\n",
    "    # That's why we can use the below..\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('dropout:0')\n",
    "\n",
    "    # tf.identity(infer_logits,\"logits\") establishes this relationship\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')     \n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
