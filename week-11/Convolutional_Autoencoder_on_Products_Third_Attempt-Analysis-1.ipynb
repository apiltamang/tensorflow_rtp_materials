{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n",
    "\n",
    "Reflections:\n",
    "In the following code, I will attempt to do some basic analysis using the auto-encoder's encoding layers, so as to measure some form of similarity of different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xavier_wts = tf.contrib.layers.xavier_initializer()\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getEncoder(images, is_training = True, reuse=False):\n",
    "    ### Encoder  \n",
    "    with tf.variable_scope(\"encoder\",reuse=reuse):\n",
    "\n",
    "        # input: 224 * 224 * num_channels    \n",
    "        x = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=5, strides=2, activation=None, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x = tf.layers.batch_normalization(inputs=x, training=is_training)\n",
    "        x = tf.maximum(alpha * x, x)\n",
    "\n",
    "        #print(x.shape.as_list())\n",
    "\n",
    "        # input: 112 * 112 * _   \n",
    "        x = tf.layers.conv2d(inputs=x, filters=64, kernel_size=5, strides=2, activation=None, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x = tf.layers.batch_normalization(inputs=x, training=is_training)\n",
    "        x = tf.maximum(alpha * x, x)\n",
    "        #print(x.shape.as_list())\n",
    "\n",
    "        # input: 56 * 56 * _\n",
    "        x = tf.layers.conv2d(inputs=x, filters=128, kernel_size=5, strides=2, activation=None, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x = tf.layers.batch_normalization(inputs=x, training=is_training)\n",
    "        x = tf.maximum(alpha * x, x)\n",
    "        #print(x.shape.as_list())\n",
    "\n",
    "        # input: 28 * 28 * _\n",
    "        x = tf.layers.conv2d(inputs=x, filters=256, kernel_size=5, strides=2, activation=None, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x = tf.layers.batch_normalization(inputs=x, training=is_training)\n",
    "        x = tf.maximum(alpha * x, x)\n",
    "        #print(x.shape.as_list())\n",
    "\n",
    "        # input: 14 * 14 * _\n",
    "        x = tf.layers.conv2d(inputs=x, filters=512, kernel_size=5, strides=2, activation=None, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x = tf.layers.batch_normalization(inputs=x, training=is_training)\n",
    "        x = tf.maximum(alpha * x, x)\n",
    "        #print(x.shape.as_list())\n",
    "\n",
    "        # Output: 7 * 7 * _\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_output = getEncoder(inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(enc_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDecoder(encoder_input, is_training=True, reuse=False):\n",
    "    with tf.variable_scope(\"decoder\", reuse=reuse):\n",
    "        \n",
    "        # input: 7 * 7 * _\n",
    "        x1 = tf.image.resize_nearest_neighbor(encoder_input, (14, 14))\n",
    "\n",
    "        x2 = tf.layers.conv2d(inputs=x1, filters=256, activation=None, kernel_size=5, strides=1, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x2 = tf.layers.batch_normalization(inputs=x2, training=is_training)\n",
    "        x2 = tf.maximum(alpha * x2, x2)\n",
    "\n",
    "        # input: 14 * 14 * _ \n",
    "        x3 = tf.image.resize_nearest_neighbor(x2, (28, 28))\n",
    "        x3 = tf.layers.conv2d(inputs=x3, filters=128, activation=None, kernel_size=5, strides=1, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x3 = tf.layers.batch_normalization(inputs=x3, training=is_training)\n",
    "        x3 = tf.maximum(alpha * x3, x3)        \n",
    "\n",
    "        # input: 28 * 28 * _ \n",
    "        x4 = tf.image.resize_nearest_neighbor(x3, (56, 56))\n",
    "        x4 = tf.layers.conv2d(inputs=x4, filters=64, activation=None, kernel_size=5, strides=1, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "        x4 = tf.layers.batch_normalization(inputs=x4, training=is_training)\n",
    "        x4 = tf.maximum(alpha * x4, x4)        \n",
    "\n",
    "        # input: 56 * 56 * _ \n",
    "        x5 = tf.image.resize_nearest_neighbor(x4, (112, 112))\n",
    "        x5 = tf.layers.conv2d(inputs=x5, filters=32, activation=None, kernel_size=5, strides=1, padding='same', kernel_initializer=xavier_wts, use_bias=False )\n",
    "        x5 = tf.layers.batch_normalization(inputs=x5, training=is_training)\n",
    "        x5 = tf.maximum(alpha * x5, x5)        \n",
    "\n",
    "        # input: 112 * 112 * _\n",
    "        x5 = tf.image.resize_nearest_neighbor(x4, (224, 224))\n",
    "        x5 = tf.layers.conv2d(inputs=x5, filters=3, activation=None, kernel_size=5, strides=1, padding='same', kernel_initializer=xavier_wts, use_bias=False)\n",
    "\n",
    "        # rescale output using tanh\n",
    "        decoder_output = tf.tanh(x5)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_output = getDecoder(enc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 224, 224, 3]\n"
     ]
    }
   ],
   "source": [
    "print(dec_output.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.squared_difference(targets_, dec_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    opt = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_images = np.load('Product_Images/all_images_raw_color.npy')\n",
    "mean_imgs = np.mean(all_images)\n",
    "stddev = np.std(all_images)\n",
    "\n",
    "# rescale image so that pixels dont have the range from 0 - 255\n",
    "rescaled_images = (all_images - mean_imgs)/stddev\n",
    "\n",
    "# reshape images so that it can be fed to con2d methods\n",
    "in_images = np.reshape(rescaled_images, newshape=(-1,224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_generator(images, batch_size):\n",
    "    start_index = 0    \n",
    "    while start_index < len(images):\n",
    "        end_index = start_index + batch_size\n",
    "        yield images[start_index:end_index,:,:,:]\n",
    "        start_index = end_index\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(in_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(in_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = in_images[0:4000]\n",
    "valid_images = in_images[4000:4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(valid_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"color_train_images_set.npy\",train_images)\n",
    "np.save(\"color_valid_images_set.npy\",valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images from binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = np.load(\"color_train_images_set.npy\")\n",
    "valid_images = np.load(\"color_valid_images_set.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_epochs = 0\n",
    "end_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch: 0\n",
      "  train_loss: 0.471464\n",
      "  train_loss: 0.349741\n",
      "  train_loss: 0.395871\n",
      "  valid loss:  0.344243\n"
     ]
    }
   ],
   "source": [
    "for epoch_n in range(start_epochs, end_epochs):\n",
    "    print(\"starting epoch: \"+str(epoch_n))\n",
    "    counter = 0\n",
    "    \n",
    "    for img in image_generator(train_images, batch_size):\n",
    "        counter +=1\n",
    "        \n",
    "        _ , train_loss = sess.run([opt, loss], feed_dict={inputs_:img, targets_:img})        \n",
    "\n",
    "        if(counter%10==0):\n",
    "            print(\"  train_loss: \"+str(train_loss))\n",
    "            \n",
    "    # validation at the end of an epoch. For now, take only a subset of the total validation set.    \n",
    "    valid_loss = sess.run(loss, feed_dict={inputs_:valid_images[0:256], targets_:valid_images[0:256]})\n",
    "        \n",
    "    print(\"  valid loss: \", str(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some images for visual validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_imgs = train_images[50:60]\n",
    "\n",
    "# draw the image\n",
    "# make plots\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "reconstructed = sess.run(dec_output, feed_dict={inputs_: show_imgs})\n",
    "for images, row in zip([show_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((224, 224,3)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_imgs = valid_images[20:30]\n",
    "\n",
    "# draw the image\n",
    "# make plots\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "\n",
    "reconstructed = sess.run(dec_output, feed_dict={inputs_: show_imgs})\n",
    "for images, row in zip([show_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((224, 224,3)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Analysis of Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_deep_conv = 49\n",
    "num_deep_filters = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unrolled = tf.reshape(enc_output, [-1, num_deep_filters, size_deep_conv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you need to feed the array of images you want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_images = in_images[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_vec = sess.run(unrolled, feed_dict={inputs_ : feed_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCosTheta(vec1, vec2):\n",
    "    dot_prod = dot(vec1, vec2)\n",
    "    mag_prod = norm(vec1) * norm(vec2)\n",
    "    if mag_prod == 0.:\n",
    "        return 0.\n",
    "    else:\n",
    "        return dot_prod/mag_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgSimilarity(prod1, prod2):\n",
    "    sum = 0.\n",
    "    for i in range(num_deep_filters):\n",
    "        vec1 = unrolled_vec[prod1,:,i]\n",
    "        vec2 = unrolled_vec[prod2,:,i]\n",
    "        cos_theta = evalCosTheta(vec1, vec2)\n",
    "        sum += cos_theta\n",
    "    \n",
    "    return sum/num_deep_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "\n",
    "For preliminary examination:\n",
    "- Evaluate unrolled_vec for the entire set of trained and validation images\n",
    "- Then for any chosen image, display the 10 most similar images.\n",
    "\n",
    "Consider:\n",
    "- Keep track of which images were pushed to train, and which for valid. sets. Maybe the images that are expected to show up as being similar were pushed to the validation set, thus not being identifiable as being a similar image ?? For a really good model (i.e able to reproduce go0d quality images), I don't see this as being a very big problem. This is because in most prelim. experiments, the model is able to reproduce valid. images with about near-equal quality as the model would reproduce images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753311586968\n"
     ]
    }
   ],
   "source": [
    "print(getAvgSimilarity(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
