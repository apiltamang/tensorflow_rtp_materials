{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week-2: Create a simple multi-layer network using pure tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the input data (only the first 'n' labels) from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the number of labels we will be using\n",
    "n_labels = 10\n",
    "n_features = 784\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Variables.\n",
    "\n",
    "Last week we learned about 'Constants' and 'placeholders' in tensorflows. Remember that constants cannot change in tensorflow, while placeholders are dummy variables, which can be input during a session.run() call. Tensorflow variables, additionally, are a critical piece of the tensorflow library. They are used for weights and biases which need to be updated internally while learning a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weights_hidden = tf.Variable(tf.random_normal([n_features, n_hidden], stddev=1), name=\"weights_hidden\")\n",
    "\n",
    "weights_out = tf.Variable(tf.random_normal([n_hidden, n_labels], stddev=1), name=\"weights_out\")\n",
    "\n",
    "biases_hidden = tf.Variable(tf.random_normal([n_hidden], stddev=1), name=\"biases_hidden\")\n",
    "\n",
    "biases_out = tf.Variable(tf.random_normal([n_labels], stddev=1), name=\"biases_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define placeholders for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = tf.placeholder(\"float\",[None, n_features])\n",
    "labels = tf.placeholder(\"float\",[None, n_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the operations that will define the algorithm to pursue.\n",
    "\n",
    "This might be better accomplished via writing a pseudo-code of some sort, before you even write anything. As with anything else, be sure to know the problem you're trying to solve well before attempting to code it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluate the linear mat-vec product along with addition of a bias term\n",
    "$$ x_h = x.W_{x -> h} + b_{h}$$\n",
    "$$ x_h = relu (x_h) $$\n",
    "$$ x_o = x.W_{h -> o} + b_{o}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_inputs = tf.add(tf.matmul(features,weights_hidden),biases_hidden)\n",
    "\n",
    "prediction_hidden = tf.nn.relu(hidden_inputs)\n",
    "\n",
    "hidden_outputs = tf.add(tf.matmul(prediction_hidden,weights_out),biases_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the logits \n",
    "\n",
    "$$ z_i = \\frac{e^{x_{o,i}}}{\\sum_i{e^{x_{o,i}} }}$$\n",
    "\n",
    "where i is the output from the i-th neuron in the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax\n",
    "logits = tf.nn.softmax(hidden_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Then, evaluate the cross_entropy term\n",
    "$$ \\phi_{k} = - { \\sum_{i} y_{k,i} . \\log{z_{k,i}}} $$\n",
    "\n",
    "where k=1,2,...N (Number of training samples), and i= i-th output in softmax layer for a sample **k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross entropy\n",
    "# This quantifies how far off the predictions were.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finally evaluate the mean of the entropy term, which will be used as the loss\n",
    "$$ \\psi = \\frac{1}{N} \\sum_{k}^{N} \\phi_k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run the model under a session, but first define the optimizer to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "# This is the method used to train the model\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations per epochs:  1718\n"
     ]
    }
   ],
   "source": [
    "#Define some basic model hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "num_examples = mnist.train.num_examples\n",
    "iterations_per_epochs = num_examples//batch_size\n",
    "\n",
    "# Rate at which the weights are changed\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(\"iterations per epochs: \",iterations_per_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  train_loss:  2.42925  val_loss:  2.39749\n",
      "epoch:  1  train_loss:  2.45836  val_loss:  2.39455\n",
      "epoch:  2  train_loss:  2.39761  val_loss:  2.39831\n",
      "epoch:  3  train_loss:  2.30658  val_loss:  2.29238\n",
      "epoch:  4  train_loss:  2.36702  val_loss:  2.36449\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for _ in range(iterations_per_epochs):\n",
    "\n",
    "            #Get the training x and y values, and run the \"optimizer\" and \"loss\" computational nodes\n",
    "\n",
    "            x_train, y_train = mnist.train.next_batch(batch_size)            \n",
    "            _ , loss_val = sess.run([optimizer, loss], feed_dict={features: x_train, labels: y_train})\n",
    "\n",
    "            '''\n",
    "            IMPORTANT: do NOT run the optimizer here. We ONLY want to evaluate the loss.\n",
    "            Get the validation x and y values, and run the session against the \"loss\" computational node\n",
    "            '''\n",
    "\n",
    "            x_valid, y_valid = mnist.valid.next_batch(batch_size)\n",
    "            val_loss = sess.run(loss, feed_dict={features: x_valid, labels: y_valid})\n",
    "            \n",
    "        print(\"epoch: \",epoch, \" train_loss: \",loss_val, \" val_loss: \",val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
