{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from \n",
    "\n",
    "https://github.com/tensorflow/tensorflow/examples/tutorials/mnist/mnist_deep.py\n",
    "    \n",
    "see also \n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "Specifically we're taking the deep MNIST tutorial, turning it into a notebook, and making it tensorboard-friendly.  Turning it into a notebook means unpacking it--no non-TF function calls--which makes it very easy to put print statements in wherever and see what is going on.  The tensorboard calls attempt to mirror as closely as possible what is done in this excellent tutorial video\n",
    "\n",
    "https://www.youtube.com/watch?v=eBbEDRsCmv4\n",
    "\n",
    "and associated code.  For questions email Matt, mattphillipsphd@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically there are four aspects of using Tensorboard illustrated in this simple example.\n",
    "\n",
    "1) Use name scopes (\"with tf.name_scope(...)\").  Tensorboard graphs are impossible to read without them.  Name scopes tell Tensorboard about the hierarchical and graphical structure of your model.\n",
    "\n",
    "2) Summary scalars.  E.g., the progression of accuracy and loss across time.\n",
    "\n",
    "3) Distributions of parameter and layer output values across time.\n",
    "\n",
    "4) Embeddings.  PCA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the log directory: You will subsequently launch tensorboard with the command\n",
    "# tensorboard --log-dir=<PATH TO LOGDIR>\n",
    "\n",
    "LOGDIR = \"tf_graphs/mnist_example/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str,\n",
    "                  default='/tmp/tensorflow/mnist/input_data',\n",
    "                  help='Directory for storing input data')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default values obtained 96% classification accuracy after 10 iterations\n",
    "\n",
    "conf = dict()\n",
    "\n",
    "conf[\"conv_window1\"] = 5             # Default: 5\n",
    "conf[\"conv_window2\"] = 5             # Default: 5\n",
    "conf[\"num_feat_maps1\"] = 32          # Default: 32\n",
    "conf[\"num_feat_maps2\"] = 64          # Default: 64\n",
    "conf[\"downsampling1\"] = 2            # Default: 2 \n",
    "conf[\"downsampling2\"] = 2            # Default: 2\n",
    "conf[\"num_dense_feats\"] = 1024       # Default: 1024\n",
    "conf[\"num_labels\"] = 10              # Default: 10\n",
    "\n",
    "conf[\"lr\"] = 1e-4                    # Default: 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x\")\n",
    "\n",
    "# x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "# number of pixels in a standard MNIST image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape to use within a convolutional neural net.\n",
    "# Last dimension is for \"features\" - there is only one here, since images are\n",
    "# grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "with tf.name_scope(\"conv1\"):\n",
    "    with tf.name_scope(\"convolve\"):\n",
    "        shape1 = [conf[\"conv_window1\"], conf[\"conv_window1\"], 1, conf[\"num_feat_maps1\"]]\n",
    "        W_conv1 = tf.Variable( tf.truncated_normal(shape1,\n",
    "                                                   stddev=0.1) )\n",
    "        b_conv1 = tf.Variable( tf.constant(0.1, \n",
    "                                           shape=[ conf[\"num_feat_maps1\"] ]) )\n",
    "\n",
    "        conv2d1 = tf.nn.conv2d(x_image, \n",
    "                               W_conv1,\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        # a 2d convolution layer with full stride.\n",
    "\n",
    "        h_conv1 = tf.nn.relu(conv2d1 + b_conv1)\n",
    "    \n",
    "        # Here we record distributions\n",
    "        tf.summary.histogram(\"weights\", W_conv1)\n",
    "        tf.summary.histogram(\"biases\", b_conv1)\n",
    "        tf.summary.histogram(\"activations\", h_conv1)\n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    with tf.name_scope(\"pool\"):\n",
    "        ds_shape1 = [1, conf[\"downsampling1\"], conf[\"downsampling1\"], 1]\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1, \n",
    "                                 ksize=ds_shape1,\n",
    "                                 strides=ds_shape1,\n",
    "                                 padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second convolutional layer -- maps 32 feature maps to 64.\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    with tf.name_scope(\"convolve\"):\n",
    "        shape2 = [conf[\"conv_window2\"], conf[\"conv_window2\"], conf[\"num_feat_maps1\"], conf[\"num_feat_maps2\"]]\n",
    "        W_conv2 = tf.Variable( tf.truncated_normal(shape2,\n",
    "                                                   stddev=0.1) )\n",
    "        b_conv2 = tf.Variable( tf.constant(0.1,\n",
    "                                           shape=[ conf[\"num_feat_maps2\"] ]) )\n",
    "\n",
    "        conv2d2 = tf.nn.conv2d(h_pool1,\n",
    "                               W_conv2,\n",
    "                               strides=[1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        # a 2d convolution layer with full stride.\n",
    "\n",
    "        h_conv2 = tf.nn.relu(conv2d2 + b_conv2)\n",
    "    \n",
    "        # Here we record distributions\n",
    "        tf.summary.histogram(\"weights\", W_conv2)\n",
    "        tf.summary.histogram(\"biases\", b_conv2)\n",
    "        tf.summary.histogram(\"activations\", h_conv2)\n",
    "    \n",
    "    # Second pooling layer.\n",
    "    with tf.name_scope(\"pool\"):\n",
    "        ds_shape2 = [1, conf[\"downsampling2\"], conf[\"downsampling2\"], 1]\n",
    "        h_pool2 = tf.nn.max_pool(h_conv2, \n",
    "                                 ksize=ds_shape2,\n",
    "                                 strides=ds_shape2,\n",
    "                                 padding='SAME')\n",
    "\n",
    "num_pool2_out = int( h_pool2.shape[1] * h_pool2.shape[2] * h_pool2.shape[3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully connected layer -- after 2 round of downsampling, our 28x28 image\n",
    "# is down to 7x7x64 feature maps in the default configuration -- maps this to 1024 features.\n",
    "with tf.name_scope(\"fc\"):\n",
    "    fc_shape_in = [num_pool2_out, conf[\"num_dense_feats\"]]\n",
    "    W_fc1 = tf.Variable( tf.truncated_normal(fc_shape_in,\n",
    "                                             stddev=0.1) )\n",
    "    b_fc1 = tf.Variable( tf.constant(0.1,\n",
    "                                     shape=[ conf[\"num_dense_feats\"] ]) )\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, num_pool2_out])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "# features.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# keep_prob is a scalar placeholder for the probability of dropout.\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map the 1024 features to 10 classes, one for each digit\n",
    "shape_out = [conf[\"num_dense_feats\"], conf[\"num_labels\"]]\n",
    "W_fc2 = tf.Variable( tf.truncated_normal(shape_out, stddev=0.1) )\n",
    "b_fc2 = tf.Variable( tf.constant(0.1, shape=[ conf[\"num_labels\"] ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# y is a tensor of shape (N_examples, 10), with values\n",
    "# equal to the logits of classifying the digit into one of 10 classes (the\n",
    "# digits 0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network, and add performance variables to track over time:\n",
    "\n",
    "For tensorboard, this means \n",
    "\n",
    "    1) Adding tf.summary.scalar statements.\n",
    "    2) Creating a summary object ('summ') at the end\n",
    "    3) Below, running the session passing in the 'summ' argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, conf[\"num_labels\"]], name=\"labels\")\n",
    "\n",
    "with tf.name_scope(\"xent\"):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    tf.summary.scalar(\"xent\", cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer( conf[\"lr\"] ).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "   \n",
    "summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start the session.  We need to add a writer to record the session graph and embeddings.\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Add the session graph.  Prior proper use of name scopes is critical for this.\n",
    "hparam =  \"lr_%.0E,%d\" % (conf[\"lr\"], conf[\"num_dense_feats\"])\n",
    "writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings to use in PCA and t-SNE visualization.  These use the outputs of the last convolutional layer, not the fully connected layers.  This also critically requires that model *checkpoints* be saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_input = tf.reshape(h_pool2, [-1, num_pool2_out])\n",
    "embedding_size = num_pool2_out\n",
    "embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "assignment = embedding.assign(embedding_input)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell is 'cheating', it uses images/data created by Dandelion Mane\n",
    "\n",
    "config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "embedding_config = config.embeddings.add()\n",
    "embedding_config.tensor_name = embedding.name\n",
    "embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "# Specify the width and height of a single thumbnail.\n",
    "embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 100, training accuracy 0.8\n",
      "step 200, training accuracy 0.86\n",
      "step 300, training accuracy 0.88\n",
      "step 400, training accuracy 0.92\n",
      "step 500, training accuracy 0.94\n",
      "step 600, training accuracy 0.94\n",
      "step 700, training accuracy 1\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.98\n",
      "step 1000, training accuracy 0.94\n",
      "test accuracy 0.9612\n"
     ]
    }
   ],
   "source": [
    "#Execute the training\n",
    "\n",
    "for i in range(1001):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 5 == 0:\n",
    "#        train_accuracy = accuracy.eval(session=sess, feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        writer.add_summary(s, i)\n",
    "    if i % 100 == 0:\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "    # Here is where we save the checkpoint for the embedding visualization\n",
    "    if i % 500 == 0:\n",
    "        sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y_: mnist.test.labels[:1024]})\n",
    "        saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "    train_step.run(session=sess, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print('test accuracy %g' % accuracy.eval(session=sess, feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: \n",
    "\n",
    "1) Augment this notebook to use and track test-set as well as training-set accuracy.\n",
    "\n",
    "2) Add more and different layers, look at graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
